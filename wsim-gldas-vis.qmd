---
title: "WSIM-GLDAS Dataset Exploration and Visualizations"
author: 
  - "Joshua Brinks"
  - "Elaine Famutimi"
date: "December, 6 2023"
bibliography: wsim-gldas-references.bib
---

## Overview

In our previous lesson, _Acquiring and Pre-Processing the WSIM-GLDAS Dataset_, we downloaded components of WSIM-GLDAS from SEDAC, subsetted the data using vector boundaries from geoBoundaries, performed a visual check, and wrote the file to disk. In this lesson, we will extend our work with WSIM-GLDAS by introducing an additional integration period, calculating simple summary statistics, integrating WSIM-GLDAS with the Gridded Population of the World, and developing more complex visualizations.

## Learning Objectives

After completing this lesson, you should be able to:

-   Subset data for a region and time period of interest.
-   Summarize raster data with zonal extractions.
-   Perform visual exploration with histograms and time series figures.
-   Use different plotting functions to make these maps.
-   Integrate multiple spatial datasets to perform more complex analyses.

## Introduction

::: column-margin
::: {.callout-tip style="color: #5a7a2b;"}

## Coding Review

This lesson uses the [`stars`](), [`lubridate`](), [`exactextractr`](), [`ggplot`](https://ggplot2.tidyverse.org/), and [`terra`](https://rspatial.org/pkg/) packages. If you'd like to learn more about the functions used in this lesson you can use the help guides on their package websites.
:::
:::

## Load Data

We'll being with the WSIM-GLDAS 12 month integration anomaly file from SEDAC and quickly subset it to the Continental United States (CONUSA). We can reduce our memory overhead by reading in just the `'deficit'` attribute from the WSIM-GLDAS composite 12 month integration file.

```{r warning=FALSE}
#| code-fold: true
wsim_gldas <- stars::read_stars("composite_12mo.nc", proxy = FALSE, sub = 'deficit')

print(wsim_gldas)
```

For this exercise we want to explore droughts in the continental United States for the years 2000-2014. In the 12 momth integration dataset, each monthly time step is an average of the previous 12 months. Therefore, if we wish to view a snapshot of drought for a given year we need to use the December timestep for that year. We can create a sequence of dates starting with December 2000 to get every year until 2014.

```{r warning=FALSE}
# generate a vector of dates for subsetting
keeps<-seq(lubridate::ymd("2000-12-01"),
           lubridate::ymd("2014-12-01"), 
           by = "year")

# filter using that vector
wsim_gldas <- dplyr::filter(wsim_gldas, time %in% keeps)
print(wsim_gldas)

```

Now can subset the WSIM-GLDAS dataset using the USA country boundary from geoBoundaries.

```{r warning=FALSE}
#directly acquire the boundary from geoBoundaries API
usa <- httr::GET("https://www.geoboundaries.org/api/current/gbOpen/USA/ADM1/")
usa <- httr::content(usa)
usa <- sf::st_read(usa$gjDownloadURL)

# remove everything not part of CONUSA
drops<-
  c("Alaska", "Hawaii", 
    "American Samoa",
    "Puerto Rico",
    "Commonwealth of the Northern Mariana Islands", 
    "Guam", 
    "United States Virgin Islands")

usa<-usa[!(usa$shapeName %in% drops),]
wsim_gldas<-wsim_gldas[usa]
```

Now we'll verify the pre-processing steps with `print()`. The raster object now contains 15 timesteps from December 2000 to December 2014.

```{r}
print(wsim_gldas)
```

## Annual CONUSA Time Series

We can start our visual exploration of annual drought in the CONUSA with a time series plot depicting each year in the dataset.

```{r warning = FALSE, message = FALSE}
# the histogram can be studied to properly choose breaks. Breaks chosen are [-60, -40, -20, 0, 20, 40, 50] from studying the histogram.
ggplot2::ggplot(usa)+
  stars::geom_stars(data = wsim_gldas)+
  ggplot2::coord_equal()+
  ggplot2::facet_wrap(~time)+
  ggplot2::geom_sf(fill = NA)+
  ggplot2::scale_fill_stepsn(
    colors = c(
    '#9B0039',
    # -50 to -40
    '#D44135',
    # -40 to -20
    '#FF8D43',
    # -20 to -10
    '#FFC754',
    # -10 to -5
    '#FFEDA3',
    # -5 to -3
    '#FFF4C7'), breaks = c(-60, -50, -40, -20, -10,-5, 0))+
  ggplot2::theme_minimal()+
  ggplot2::theme(
    axis.title.x=ggplot2::element_blank(),
    axis.text.x=ggplot2::element_blank(),
    axis.ticks.x=ggplot2::element_blank(),
    axis.title.y=ggplot2::element_blank(),
    axis.text.y=ggplot2::element_blank(),
    axis.ticks.y=ggplot2::element_blank())
```

There are several significant drought events throughout 2000-2014. The southeast in 2000, southwest in 2002, the majority of the western 3rd in 2007, Texas-Oklahoma in 2011, Montana-Wyoming-Colorado in 2012, and the entirety of the California coast in 2014. The droughts of 2012 and 2011 are particularly severe and widespread with return periods greater than 50 covering multiple states. Based on historical norms, we should only expect droughts this strong every 50-60 years!

## Monthly Time Series

We can get a better look at these drought events by using the 1 month composite WSIM-GLDAS dataset and subsetting the data with a smaller spatial extent. Let's examine the 2014 California drought.

To reduce our memory overhead, we'll start by removing the 12 month composite object.

```{r}
rm(wsim_gldas)
```

Now let's load the composite 1 month file from SEDAC.

```{r}
gc()
wsim_gldas_1mo <- stars::read_stars("composite_1mo.nc", sub = 'deficit', proxy = FALSE)

print(wsim_gldas_1mo)
```

Once again, we'll subset the time dimension for our period of interest. This time we want every month for 2014.

```{r}
# generate a vector of dates for subsetting
keeps<-seq(lubridate::ymd("2014-01-01"),
           lubridate::ymd("2014-12-01"), 
           by = "month")

# filter using that vector
wsim_gldas_1mo <- dplyr::filter(wsim_gldas_1mo, time %in% keeps)
print(wsim_gldas_1mo)
```

We're down to 12 monthly timesteps. Lets zoom in on California and see how this drought progressed over the course of the year.

```{r warning = FALSE, message = FALSE}
california<-usa[usa$shapeName=="California",]
wsim_gldas_california <- wsim_gldas_1mo[california]

# monthly plots of california
ggplot2::ggplot(california)+
  stars::geom_stars(data = wsim_gldas_california)+
  ggplot2::coord_equal()+
  ggplot2::facet_wrap(~time)+
  ggplot2::geom_sf(fill = NA)+
  ggplot2::scale_fill_stepsn(
    colors = c(
    '#9B0039',
    # -50 to -40
    '#D44135',
    # -40 to -20
    '#FF8D43',
    # -20 to -10
    '#FFC754',
    # -10 to -5
    '#FFEDA3',
    # -5 to -3
    '#FFF4C7',
    # -3 to 0
    '#FFFFFF'), breaks = c(-60, -50, -40, -20, -10,-5,-3, 0))+
  ggplot2::theme_minimal()+
  ggplot2::theme(
    axis.title.x=ggplot2::element_blank(),
    axis.text.x=ggplot2::element_blank(),
    axis.ticks.x=ggplot2::element_blank(),
    axis.title.y=ggplot2::element_blank(),
    axis.text.y=ggplot2::element_blank(),
    axis.ticks.y=ggplot2::element_blank())
```
This is a startling series of maps. The entire state faced massive deficits in January and February. This was followed by extreme droughts throughout the entire western half of the state in May-August. Although northern and eastern California saw some relief by September, southwest California continued to suffer through December.

## Monthly Histograms

We can get a different view of the data by creating a histogram of the deficit anomalies. We can extract the data from the raster time series and create a data frame of values that are easier to manipulate into a histogram.

```{r}
# Pull the values
deficit_hist <-  
  wsim_gldas_california |>
  as.data.frame(wsim_gldas_california$deficit)

# Rename the date for just the months
deficit_hist$time<-lubridate::month(deficit_hist$time, label = TRUE, abbr = FALSE)
# remove the NA values
deficit_hist<-stats::na.omit(deficit_hist)

ggplot2::ggplot(deficit_hist, ggplot2::aes(deficit))+
  ggplot2::geom_histogram(binwidth = 6, fill = "#325d88")+
  ggplot2::facet_wrap(~time)+
  ggplot2::theme_minimal()
```
This starts to quantify what our eyes were telling us with the time series maps. The number of raster cells under a 60 year deficit is pretty incredible in most months. 

## Zonal Summaries

The previous section digs deeper into the 2014 California drought, but mostly just examines the state as a whole. Although we have a sense of what's happening in different cities or counties by looking at the time series plots, they do not provide quantitative summaries of specific areas. 

Zonal statistics are one way to summarize the cells of a raster layer that lie within the boundary of another data layer; this may be another raster or vector (shapefile) layer. For example, summarizing deficit return periods with another raster depicting land cover type or a vector boundary/shapefile of countries, states, or counties. 

For the purposes of this lesson we'll begin by calculating the mean deficit return period by California counties. We'll start by retrieving a layer with California counties from geoBoundaries.

```{r}
# get the adm2 (county) level data for the USA
cali_counties <- httr::GET("https://www.geoboundaries.org/api/current/gbOpen/USA/ADM2/")
cali_counties <- httr::content(cali_counties)
cali_counties <- sf::st_read(cali_counties$gjDownloadURL)
# geoBoundaries does not list which counties belong to which state so you need to run an intersection
cali_counties<-sf::st_intersection(cali_counties, california)
plot(sf::st_geometry(cali_counties))
```
That looks pretty good. The ADM1 (national) and ADM2 (province/state) data for many countries in geoBoundaries does not always match up correctly. However, boundaries for the USA are pretty consistent across ADM0/ADM1/ADM2 so simple intersection procedures are less likely to bleed over into neighboring states.

We will perform our zonal statistics using the exactextractr package. It is the fastest, most accurate, and most flexible zonal statistics tool for the R programming language, but it currently has no default methods for the `stars` package, so we'll switch to `terra` for this portion of the lesson.

```{r}
wsim_gldas_1mo<-terra::sds("composite_1mo.nc")
wsim_gldas_1mo<-wsim_gldas_1mo["deficit"]
keeps<-seq(lubridate::ymd("2014-01-01"), lubridate::ymd("2014-12-01"), by = "month")

wsim_gldas_1mo<-wsim_gldas_1mo[[terra::time(wsim_gldas_1mo) %in% keeps]]
names(wsim_gldas_1mo) <- keeps
print(wsim_gldas_1mo)
```


```{r warning=FALSE, message=FALSE}
cali_county_summaries<-
  exactextractr::exact_extract(wsim_gldas_1mo, cali_counties, 'mean')
names(cali_county_summaries)<-lubridate::month(keeps, label = TRUE, abbr = FALSE)
cali_counties<-cbind(cali_counties, cali_county_summaries)

```

```{r warning=FALSE, message=FALSE}
plot(cali_counties[c(11:23)],
     pal = c(
    '#9B0039',
    # -50 to -40
    '#D44135',
    # -40 to -20
    '#FF8D43',
    # -20 to -10
    '#FFC754',
    # -10 to -5
    '#FFEDA3',
    # -5 to -3
    '#FFF4C7'), breaks = c(-61, -50, -40, -20, -10,-5, 5), key.pos = 1)
```


## Locations of Interest

To this point we've explored data at the national or state level, however, we can also extract deficit return periods for a point location

```{r}
# vector of coordinates Missouri River Basin
coords <- c(34.0549, -118.2426)

# create a stars object with point coordinates where 'sf::st_sfc()' is used to create a geometry list column and add a coordinate reference system. 'sf::st_points()' is within that function to turn the coords vector into a point. 'sf:: st_crs()' and 'stars::st_dimensions()'  respectively retrieves the coordinate reference system and the dimensions of wsim_gldas_1mo to ensure that the final values in point_stars is the same reference system and shape. 

point_stars <- sf::st_sfc(sf::st_point(coords), crs = sf::st_crs(wsim_gldas_1mo),
                      dim = names(stars::st_dimensions(wsim_gldas_1mo)))
```

Use `stars::extract` to extract raster values in the stack at the point location.

```{r}
extracted_vals <- stars::st_extract(wsim_gldas_1mo, point_stars)
```

The resulting data frame of time series values should be inspected. It may also need to be converted from wide format to long format so it may be plotted in `ggplot`. Use either pivot wider/longer from `dplyr` or cast/melt from `data.table`.

```{r}
# convert to dataframe
extracted_df <- as.data.frame(extracted_vals)

print(extracted_df)
```

Once in the proper format, plot using `ggplot`.

```{r}
ggplot2::ggplot(extracted_df, ggplot2::aes(x = time, y = deficit)) + ggplot2::geom_line()
```

\*\*Creating a time series similar to this can be useful in water resources management, which is the process of planning and managing water resources across all water uses. One can use time series analysis in models to predict droughts and flooding. It helps water resource managers determine who is able to use the available water.

However, sometimes there can be issues in water resources management, especially across separate divisions. In 2012, Missouri experienced a severe drought that was the worst in more than 50 years, which negatively impacted the Missouri River Basin. A river basin is described as a total area of land that is drained by a river and it's tributaries, while tributaries are rivers or streams that flow into a larger river.

The Army Corps of Engineers reduced the flow from Gavins Point Dam, a Missouri River reservoir, to protect the upper Missouri River basin in November 2012. That caused tension between water demand in the basin and transport demand on the Mississippi River, since the Missouri River flows into the Mississippi River. Additionally, Mississippi was also experiencing a drought.[@USAToday2012]

The Missouri River Basin functions for drinking water, irrigation and industrial purposes, and power production, while the Mississippi River benefits industries that rely on the river for trade and transport, which is important for the region's economy. Since both rivers are managed by difference offices, it was hard to determine a water resources solution that wouldn't impact both rivers.[@WorldView2012]

## Population Exposure Plot

Use Gridded Population of the World and `exactextractr` to determine the number of people exposed to a given anomaly for each month of the year [@Baston2023].

**Gridded Population of the World** is a dataset group in SEDAC which models the distribution of the global human population through counts and densities on a raster. It displays the global distribution of the human population on a continuous surface. There are other versions of Gridded Population of the World, and the one used in this tutorial is the fourth version. Past versions like the third one, has been used for disaster impacts and environmental change.

This version has been updated using 2010 census data and is built from the previous versions. The accuracy of Gridded Population of the World version 4 was improved due to improvements in technology that allowed the Census bureaus to effortlessly distribute their results to the public using electronic and online formats.

These estimates are available for the years of 2000, 2005, 2015 and 2020. The Population Count dataset will be used for this example, and can be downloaded from the NASA [SEDAC](https://sedac.ciesin.columbia.edu/data/collection/gpw-v4) website [@CIESIN2018]. For this upcoming example, you will need to download the .tif file for the year of 2020, at the 15 minute resolution.

[exactextractr](https://github.com/isciences/exactextractr) [@Baston2023] is an R package that summarizes raster values over groupings, or zones, also known as zonal statistics. Zonal statistics help in assessing the statistical characteristics of a certain region. The package can be used to sum all the cells in a polygon, i.e. a region; other functions include computing the mean, median, and other types of statistics.

Load in GPW data using `terra` and the `exactextractr` package

```{r}
library(exactextractr)

gpw1 <- terra::rast("gpw_v4_population_count_rev11_2020_15_min.tif")
print(gpw1)
```

Perform the time series zonal summary.

This might be a bit tricky; been a while for me. Have to look up the proper code. Dan has good examples on the exactextractr package website.

Resulting data.frame will probably need to be transformed to long (just like before), so it can be plotted.

```{r}
extracted_mean <- exact_extract(gpw1, usa, 'mean')
print(extracted_mean)

extracted_mean_df <- as.data.frame(extracted_mean)
print(extracted_mean_df)
```

You will have to use the function "cbind" to bind together the extracted mean of each state along with the states in the USA boundary.

```{r}
joined_var = cbind(usa, extracted_mean)
print(joined_var)

joined_var_df = as.data.frame(joined_var)
print(joined_var_df)
```

Now plot the data in ggplot. Add a title and label the axis. I have some existing code I can pull to help with the plotting--or at least make it fancy.

```{r}
ggplot2::ggplot(data = joined_var_df, ggplot2::aes(x = shapeName, y = extracted_mean, group = 1)) + ggplot2::geom_line() +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 90, hjust = 1)) +
  ggplot2::theme(plot.title =  ggplot2::element_text(hjust = 0.5)) +
  ggplot2::ggtitle("Population Mean per State") + 
  ggplot2::xlab("State") + 
  ggplot2::ylab("Mean")
```

## In this Lesson, You Learned...

Congratulations! Now you should be able to:

-   Download geoboundaries data using the httr::GET() method.

-   Identify hot spots of precipitation data and select these hotspots for further analysis by subsetting.

-   Extract data from a pixel by using the extract function from the Stars library.

-   Perform zonal statistics of population data using the extractexactr library.

## Conclusion

The insights drawn from various data sources, including remote sensing data, contribute to the understanding of water availability. Models like the Global Land Data Assimilation System (GLDAS) utilize satellite and ground-based observations to provide real-time, high-resolution data on land surface states and fluxes, aiding in a more comprehensive understanding of drought and flooding [@Rodell2004].
