---
title: "WSIM-GLDAS Dataset Exploration and Visualizations"
author: 
  - "Joshua Brinks"
  - "Elaine Famutimi"
date: "December, 6 2023"
bibliography: wsim-gldas-references.bib
---

## Overview

In our previous lesson, _Acquiring and Pre-Processing the WSIM-GLDAS Dataset_, we downloaded components of WSIM-GLDAS from SEDAC, subsetted the data using vector boundaries from geoBoundaries, performed a visual check, and wrote the file to disk. In this lesson, we will extend our work with WSIM-GLDAS by introducing an additional integration period, calculating simple summary statistics, integrating WSIM-GLDAS with the Gridded Population of the World, and developing more complex visualizations.

## Learning Objectives

After completing this lesson, you should be able to:

-   Subset data for a region and time period of interest.
-   Summarize raster data using zonal statistics.
-   Perform visual exploration with histograms.
-   Build choropleth maps summarizing data by vector boundaries.
-   Integrate multiple spatial datasets to perform complex analyses and construct intricate visualizations.

## Introduction

::: column-margin
::: {.callout-tip style="color: #5a7a2b;"}

## Coding Review

This lesson uses the [`stars`](https://r-spatial.github.io/stars/), [`sf`](https://r-spatial.github.io/sf/), [`dplyr`](https://dplyr.tidyverse.org/), [`lubridate`](https://lubridate.tidyverse.org/) [`exactextractr`](https://isciences.gitlab.io/exactextractr/), [`ggplot2`](https://ggplot2.tidyverse.org/), [`terra`](https://rspatial.org/pkg/), and [`data.table`](https://rdatatable.gitlab.io/data.table/) packages. If you'd like to learn more about the functions used in this lesson you can use the help guides on their package websites.
:::
:::

## Load Data

We'll being with the WSIM-GLDAS 12 month integration anomaly file from SEDAC and quickly subset it to the Continental United States (CONUSA). We can reduce our memory overhead by reading in just the `'deficit'` attribute from the WSIM-GLDAS composite 12 month integration file.

```{r warning=FALSE}
wsim_gldas <- stars::read_stars("composite_12mo.nc", proxy = FALSE, sub = 'deficit')

print(wsim_gldas)
```

For this exercise we want to explore droughts in the continental United States for the years 2000-2014. In the 12 momth integration dataset, each monthly time step is an average of the previous 12 months. Therefore, if we wish to view a snapshot of drought for a given year we need to use the December timestep for that year. We can create a sequence of dates starting with December 2000 to get every year until 2014.

```{r warning=FALSE}
# generate a vector of dates for subsetting
keeps<-seq(lubridate::ymd("2000-12-01"),
           lubridate::ymd("2014-12-01"), 
           by = "year")

# filter using that vector
wsim_gldas <- dplyr::filter(wsim_gldas, time %in% keeps)
print(wsim_gldas)

```

Now can subset the WSIM-GLDAS dataset using the USA country boundary from geoBoundaries.

```{r warning=FALSE}
#directly acquire the boundary from geoBoundaries API
usa <- httr::GET("https://www.geoboundaries.org/api/current/gbOpen/USA/ADM1/")
usa <- httr::content(usa)
usa <- sf::st_read(usa$gjDownloadURL)

# remove everything not part of CONUSA
drops<-
  c("Alaska", "Hawaii", 
    "American Samoa",
    "Puerto Rico",
    "Commonwealth of the Northern Mariana Islands", 
    "Guam", 
    "United States Virgin Islands")

usa<-usa[!(usa$shapeName %in% drops),]

wsim_gldas<-wsim_gldas[usa] |>
stars::st_set_dimensions("time", values = as.character(seq(2000,2014)))
```

Now we'll verify the pre-processing steps with `print()`. The raster object now contains 15 timesteps from December 2000 to December 2014.

```{r}
print(wsim_gldas)
```

## Annual CONUSA Time Series

We can start our visual exploration of annual drought in the CONUSA with a time series plot depicting each year in the dataset.

```{r warning = FALSE, message = FALSE}
ggplot2::ggplot(usa)+
  stars::geom_stars(data = wsim_gldas)+
  ggplot2::coord_equal()+
  ggplot2::facet_wrap(~time)+
  ggplot2::geom_sf(fill = NA)+
  ggplot2::scale_fill_stepsn(
    colors = c(
    '#9B0039',
    # -50 to -40
    '#D44135',
    # -40 to -20
    '#FF8D43',
    # -20 to -10
    '#FFC754',
    # -10 to -5
    '#FFEDA3',
    # -5 to -3
    '#FFF4C7',
    # -3 to 0
    '#FFFFFF'), breaks = c(-60, -50, -40, -20, -10,-5,-3, 0))+
  ggplot2::labs(
    title="Annual Mean Deficit Anomalies for the CONUSA",
    subtitle = "Using Observed 12 Month Integrated WSIM-GLDAS Data for 2000-2014"
  )+
  ggplot2::theme_minimal()+
  ggplot2::theme(
    axis.title.x=ggplot2::element_blank(),
    axis.text.x=ggplot2::element_blank(),
    axis.ticks.x=ggplot2::element_blank(),
    axis.title.y=ggplot2::element_blank(),
    axis.text.y=ggplot2::element_blank(),
    axis.ticks.y=ggplot2::element_blank())
```

There are several significant drought events throughout 2000-2014. The southeast in 2000, southwest in 2002, the majority of the western 3rd in 2007, Texas-Oklahoma in 2011, Montana-Wyoming-Colorado in 2012, and the entirety of the California coast in 2014. The droughts of 2012 and 2011 are particularly severe and widespread with return periods greater than 50 covering multiple states. Based on historical norms, we should only expect droughts this strong every 50-60 years!

## Monthly Time Series

We can get a better look at these drought events by using the 1 month composite WSIM-GLDAS dataset and subsetting the data with a smaller spatial extent. Let's examine the 2014 California drought.

::: {.callout-tip style="color: #7d2748;"}
## Drought in the News

The California drought of 2012-2014 was the worst in 1,200 years. Analysis found that it wasn’t uncommon for California to have periods of low precipitation, but that combined with sustained record high temperatures created severe water shortages. [@WHOI2014] This drought caused problems for homeowners, and even conflicts between farmers and wild salmon!

Governor Jerry Brown declared a drought emergency and called on residents to reduce water intake by 20%. Water use went up by 8% in May of 2014 compared to 2013, in places like coastal California and Los Angeles. Due to that, the state voted to fine water-wasters up to 500 dollars. The drought also affected residents differently based on economic status. For example, in El Dorado County, located in a rural area east of Sacramento, residents were taking bucket showers. Rural residents also rely on wells, which were drying up. The federal government eventually announced a $9.7 million emergency drought aid for those areas.[@Sanders2014]

Additionally, there were thousands of adult salmon struggling to survive in the Klamath River in Northern California, where water was running low and warm due to the diversion of river flow into the Central Valley, an agricultural area that grows almond trees. Almonds are one of California’s most important crops, with the state producing 80% of the world’s almonds. However, salmon, which migrate upstream, could get a disease called gill rot, which flourishes in warm water and already killed tens of thousands of Chinook in 2002. This disease was spreading through the salmon population again due to this water allocation, affecting local Native American tribes that rely on the fish.[@Bland2014]

:::

To reduce our memory overhead, we'll start by removing the 12 month composite object.

```{r}
rm(wsim_gldas)
```

Now let's load the composite 1 month file from SEDAC.

```{r}
gc()
wsim_gldas_1mo <- stars::read_stars("composite_1mo.nc", sub = 'deficit', proxy = FALSE)

print(wsim_gldas_1mo)
```

Once again, we'll subset the time dimension for our period of interest. This time we want every month for 2014.

```{r}
# generate a vector of dates for subsetting
keeps<-seq(lubridate::ymd("2014-01-01"),
           lubridate::ymd("2014-12-01"), 
           by = "month")

# filter using that vector
wsim_gldas_1mo <- dplyr::filter(wsim_gldas_1mo, time %in% keeps)
print(wsim_gldas_1mo)
```

We're down to 12 monthly timesteps. Lets zoom in on California and see how this drought progressed over the course of the year.

```{r warning = FALSE, message = FALSE}
california<-usa[usa$shapeName=="California",]

wsim_gldas_california <- wsim_gldas_1mo[california]
wsim_gldas_california <-
  wsim_gldas_california |>
    stars::st_set_dimensions("time", values = month.name)

# monthly plots of california
ggplot2::ggplot(california)+
  stars::geom_stars(data = wsim_gldas_california)+
  ggplot2::coord_equal()+
  ggplot2::facet_wrap(~time)+
  ggplot2::geom_sf(fill = NA)+
  ggplot2::scale_fill_stepsn(
    colors = c(
    '#9B0039',
    # -50 to -40
    '#D44135',
    # -40 to -20
    '#FF8D43',
    # -20 to -10
    '#FFC754',
    # -10 to -5
    '#FFEDA3',
    # -5 to -3
    '#FFF4C7',
    # -3 to 0
    '#FFFFFF'), breaks = c(-60, -50, -40, -20, -10,-5,-3, 0))+
  ggplot2::labs(
    title="Deficit Anomalies for California",
    subtitle = "Using Observed Monthly WSIM-GLDAS Data for 2014"
  )+
  ggplot2::theme_minimal()+
  ggplot2::theme(
    axis.title.x=ggplot2::element_blank(),
    axis.text.x=ggplot2::element_blank(),
    axis.ticks.x=ggplot2::element_blank(),
    axis.title.y=ggplot2::element_blank(),
    axis.text.y=ggplot2::element_blank(),
    axis.ticks.y=ggplot2::element_blank())
```
This is a startling series of maps. The entire state faced massive deficits in January and February. This was followed by extreme droughts throughout the entire western half of the state in May-August. Although northern and eastern California saw some relief by September, southwest California continued to suffer through December.

## Monthly Histograms

We can get a different view of the data by creating a histogram of the deficit anomalies. We can extract the data from the raster time series and create a data frame of values that are easier to manipulate into a histogram.

```{r}
# Pull the values
deficit_hist <-  
  wsim_gldas_california |>
  as.data.frame(wsim_gldas_california$deficit)

# remove the NA values
deficit_hist<-stats::na.omit(deficit_hist)

ggplot2::ggplot(deficit_hist, ggplot2::aes(deficit))+
  ggplot2::geom_histogram(binwidth = 6, fill = "#325d88")+
  ggplot2::facet_wrap(~time)+
  ggplot2::theme_minimal()
```
This starts to quantify what our eyes were telling us with the time series maps. The number of raster cells under a 60 year deficit is pretty incredible in most months. 

## Zonal Summaries

The previous section digs deeper into the 2014 California drought, but mostly just examines the state as a whole. Although we have a sense of what's happening in different cities or counties by looking at the time series plots, they do not provide quantitative summaries of specific areas. 

Zonal statistics are one way to summarize the cells of a raster layer that lie within the boundary of another data layer; this may be another raster or vector (shapefile) layer. For example, summarizing deficit return periods with another raster depicting land cover type or a vector boundary/shapefile of countries, states, or counties. 

For the purposes of this lesson we'll begin by calculating the mean deficit return period by California counties. We'll start by retrieving a layer with California counties from geoBoundaries.

```{r}
# get the adm2 (county) level data for the USA
cali_counties <- httr::GET("https://www.geoboundaries.org/api/current/gbOpen/USA/ADM2/")
cali_counties <- httr::content(cali_counties)
cali_counties <- sf::st_read(cali_counties$gjDownloadURL)
# geoBoundaries does not list which counties belong to which state so you need to run an intersection
cali_counties<-sf::st_intersection(cali_counties, california)
plot(sf::st_geometry(cali_counties))
```
That looks pretty good. The ADM1 (national) and ADM2 (province/state) data for many countries in geoBoundaries does not always match up correctly. However, boundaries for the USA are pretty consistent across ADM0/ADM1/ADM2 so simple intersection procedures are less likely to bleed over into neighboring states.

::: column-margin
::: {.callout-tip style="color: #5a7a2b;"}
## Coding Review
[exactextractr](https://github.com/isciences/exactextractr) [@Baston2023] is an R package that summarizes raster values over groupings, or zones, also known as zonal statistics. Zonal statistics help in assessing the statistical characteristics of a certain region. 
:::
:::

We will perform our zonal statistics using the exactextractr package [@Baston2023]. It is the fastest, most accurate, and most flexible zonal statistics tool for the R programming language, but it currently has no default methods for the `stars` package, so we'll switch to `terra` for this portion of the lesson.

```{r}
wsim_gldas_1mo<-terra::sds("composite_1mo.nc")
wsim_gldas_1mo<-wsim_gldas_1mo["deficit"]
keeps<-seq(lubridate::ymd("2014-01-01"), lubridate::ymd("2014-12-01"), by = "month")

wsim_gldas_1mo<-wsim_gldas_1mo[[terra::time(wsim_gldas_1mo) %in% keeps]]
names(wsim_gldas_1mo) <- keeps
print(wsim_gldas_1mo)
```


```{r warning=FALSE, message=FALSE}
cali_county_summaries<-
  exactextractr::exact_extract(wsim_gldas_1mo, cali_counties, 'mean', progress = FALSE)
names(cali_county_summaries)<-lubridate::month(keeps, label = TRUE, abbr = FALSE)
```

exactextractr will return summary statistics in the same order of the input boundary file so we can simply bind the California county names to the exactextractr output. We'll append the summary statistics to the sf object for more plotting, but we'll also make a simpler version to view as a table to inspect the raw data. We can take a quick look at the first 10 counties and their mean deficit return period for January-June.

```{r}
cali_counties<-cbind(cali_counties, cali_county_summaries)

cali_county_table<-cbind(County=cali_counties$shapeName,
                         round(cali_county_summaries))
kableExtra::kbl(cali_county_table[c(1:10),c(1:7)]) |>
    kableExtra::kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"))
```
This definitely confirms the sea of red in our exploratory maps. The data is currently in wide format, which makes for easy viewing of a time series, but more advanced programmatic visualization typically require data to be in long format (more on that later).

## County Choropleths

Now that we've inspected the raw data we can make a choropleth out of the mean deficit return period data. 

```{r warning=FALSE, message=FALSE}
plot(cali_counties[c(11:23)],
     pal = c(
    '#9B0039',
    # -50 to -40
    '#D44135',
    # -40 to -20
    '#FF8D43',
    # -20 to -10
    '#FFC754',
    # -10 to -5
    '#FFEDA3',
    # -5 to -3
    '#FFF4C7',
    # -3 to 0
    '#FFFFFF'), 
    breaks = c(-61, -50, -40, -20, -10,-5,-3, 5), 
    key.pos = 1)
```
Due to the widespread extreme drought in the raw data, the mean values do not appear much different from the raw deficit raster layer, however, choropleths can be easier for users to survey the landscape and place themselves and their lived experiences with the data.

While this paints a striking picture of widespread drought, how many people are affected by this drought? Although the land area appears rather large, if you're not familiar with the distribution of urban centers in California it can be difficult to get a sense of the direct human impact. 

## Integrating Population Data

**Gridded Population of the World** (GPW) is a dataset collection in SEDAC that models the distribution of the global human population through counts and densities in a raster format [@CIESIN2018]. We will take full advantage of exactextractr to integrate across WSIM-GLDAS, geoBoundaries, and GPW. To begin, we need to download the 15 minute 2010 population *density* GPWv4. This most closely matches our time period (2014) and the resolution of WSIM-GLDAS. Although it may seem more intuitive to use GPW's population *count* data layers, you can achieve more accurate results (especially along coastlines) by using population density in conjunction with land area estimates derived from exactextractr.

::: column-margin
::: {.callout-tip style="color: #5a7a2b;"}
## Data Review
The Gridded Population of the World Version 4 is available in multiple target metrics (e.g. counts, density), time periods (2000, 2005, 2010, 2015, 2020), and spatial resolutions (30 sec, 2.5 min, 15 min, 30 min, 60 min). Read more about GPW at the [collection home page on SEDAC](https://sedac.ciesin.columbia.edu/data/collection/gpw-v4).
:::
:::

Load in the population density layers.

```{r}
pop_dens<-terra::rast("gpw_v4_population_density_rev11_2015_15_min.tif")
print(pop_dens)
```
For this example we'll classify the WSIM-GLDAS deficit return period raster layer into 6 categories. This will make it easier to manage the output and interpret the data. 

```{r}
# set the class breaks row-wise (from, to, new label)
m <- c(0, 5, 0,
       -3, 0, -3,
       -5, -3, -5,
       -10, -5, -10,
       -20, -10, -20,
       -40, -20, -40,
       -50, -40, -50,
       -65, -50, -60)
# create the classification matrix
rclmat <- matrix(m, ncol=3, byrow=TRUE)
# classiffy the data
wsim_gldas_1mo_class <-
  terra::classify(wsim_gldas_1mo, rclmat, include.lowest = TRUE)
```


In our prior example we used exactextractr's built in `'mean'` function, but we can pass more complicated custom functions that will carryout several operations at once. The following chunks of code could be combined into a single function passed to exactextractr, but we will break it up a bit in case you want to follow along more easily. You can read more about exactextractr arguments in the package help guide. The key arguments to be aware of are the calls to:

  1. `weights = pop_dens`: summarizes each WSIM-GLDAS cell's deficit return period with the corresponding population density value. 
  2. `coverage_area = TRUE`: calculates the corresponding area of the WSIM-GLDAS raster cell that is covered by the California boundary.

  
```{r}
pop_by_rp <-
  exactextractr::exact_extract(wsim_gldas_1mo_class, california, function(df) {
    df <- data.table::setDT(df)
    }, 
  summarize_df = TRUE, 
  weights = pop_dens, 
  coverage_area = TRUE,
  include_cols = 'shapeISO', 
  progress = FALSE)

names(pop_by_rp)
```
This returns a `data.frame` with a row for every WSIM-GLDAS cell that is overlapped by the California cell. The first column returns the label of the polygon boundary where the cell is located. In this instance it's not very helpful because we used the state level California boundary, but if we passed the ADM2 boundary with counties it would provide the name of the county where the cell is located. The next 12 columns list the deficit return period classification value for the cell in each of the 12 months corresponding to the time dimension of the `wsim_gldas_1mo` raster layer.

The last two columns are `weight` and `coverage_area`. The `weight` column lists the corresponding population density value (persons per km^2) for that WSIM-GLDAS cell. Lastly, the `coverage_area` lists the total area (m^2) of the WSIM-GLDAS cell that is covered by the California boundary layer. With this information we can calculate the number of people living in this cell and relay their respective deficit return period for each of the 12 months in our WSIM-GLDAS raster object. We will need to perform a few more processing steps to prepare this `data.frame` for a time series visualization integrating all the data.

As mentioned before, we need to melt the data from wide format to long format so we can make a complex visualization in ggplot2.

```{r}
pop_by_rp <-
  data.table::melt(
    pop_by_rp,
    id.vars = c("shapeISO", "coverage_area", "weight"),
    variable.name = "month",
    value.name = "return_period")

head(pop_by_rp)
```
Now each row lists the area covered by the cell, the population density weight for the cell, the month the deficit return period cell corresponds to, and the actual deficit return period class for the cell. 

Next, we'll summarize the data by return period class. 

```{r}
pop_by_rp <-
  pop_by_rp[, .(pop_rp = round(sum(coverage_area * weight) / 1e6)), by = .(month, return_period)]
# some cells do not have deficit return period values and result in NaN--just remove
pop_by_rp <- na.omit(pop_by_rp)
head(pop_by_rp)
```

Now we have a row for every unique combination of month, return period class, and the total population. We can calculate the percent of the total population represented by this return period class with 2 more lines.

```{r}
pop_by_rp[, total_pop := sum(pop_rp), by = month]
pop_by_rp[, pop_frac := pop_rp / total_pop][, total_pop := NULL]

head(pop_by_rp)
```
Now we can put it all together into a visualization.

```{r}
# pretty month labels
pop_by_rp[,month:=lubridate::month(pop_by_rp$month, label = TRUE)]
# ggplot is easier with factors
pop_by_rp[,return_period:=as.factor(return_period)]

leg_colors<-c(
    '#9B0039',
    # -50 to -40
    '#D44135',
    # -40 to -20
    '#FF8D43',
    # -20 to -10
    '#FFC754',
    # -10 to -5
    '#FFEDA3',
    # -5 to -3
    '#fffdc7',
    # -3 to 0
    '#FFF4C7',
    # 0-3
    "#FFFFFF")

ggplot2::ggplot(pop_by_rp, 
                ggplot2::aes(x = month, 
                             y = pop_frac,
                             group = return_period, 
                             color = return_period,
                             fill = return_period
                ))+
  ggplot2::geom_area(position = 'identity', outline.type = "upper", lwd = 0.75, color = "darkgrey")+
  ggplot2::scale_color_manual(values = leg_colors)+
  ggplot2::scale_fill_manual(values = leg_colors)+
  ggplot2::ylim(0,1)+
  ggplot2::labs(title = "Monthly Fraction of Population Under Water Deficits in California During 2014",
                subtitle = "Categorized by Intensity of Deficit Return Period",
                x = "",
                y = "Fraction of Population*",
                caption = "*Population derived from Gridded Population of the World (2015)",
                color = "Return Period", fill = "Return Period", group = "Return Period", alpha = "Return Period")+
  ggplot2::theme_minimal()
```
This figure really illustrates the human impact of the 2014 drought. Nearly 100% of the population was under a 60+ year deficit in January followed by 66% in May and approximately 40% for the remainder of the summer. That is a devastating drought!

## Congratulations! In this Lesson You Learned How To...

-   Download geoboundaries ADM1 and ADM2 data directly from their API.
-   Identify hot spots of drought and select these hotspots for further analysis.
-   Summarize data by county using the exactextractr tool.
-   Integrate WSIM-GLDAS deficit, GPW population, and geoBoundaries administrative data to create complex time series visualizations.

## Lesson 3

In this lesson we explored the California drought of 2014. In our next lesson we will examine real time flood data in California using the MODIS data product.

[Lesson 3: Moderate Resolution Imaging Spectroradiometer (MODIS) Near-Real Time (NRT) flood data](https://ciesin-geospatial.github.io/TOPSTSCHOOL-module-1-water/lance-modis-nrt-global-flood-mcdwd-f3.html){.btn .btn-primary .btn role="button"}

# References