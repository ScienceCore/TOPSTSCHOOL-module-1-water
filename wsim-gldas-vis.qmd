---
title: "WSIM-GLDAS Dataset Exploration and Visualizations"
author: 
  - "Joshua Brinks"
  - "Elaine Famutimi"
date: "December, 6 2023"
---

## TO DO

  * Write the actual code and narrative.
  * Determine the region and time period of focus to draw in our use cases/human focused stories.
  * Determine the method of exploration.
    + Mimic our process?
      * 12 month integration panels of the CON USA from 2000-2014 to identify areas of interest.
      * Zoom in to locations of interest and switch to 1-month integration for the years identified in the previous step.

## Introduction

A raster is a grid of geographic data which has information stored per pixel. Vectors are a collection or a list of numbers. Vectors can be used to store coordinates, which can then be used to extract information from a pixel in the raster for analysis, such as a time series. Rasters can store many types of information as attributes, and they usually have dimensions such as latitude, longitude, and time.

The water cycle is the constant process of circulation of water on, above, and under the Earth's surface. Human interventions such as greenhouse gas emissions, land use changes, dam and reservoir development, and groundwater overexploitation have all drastically affected the natural water cycle in recent decades. Human interventions in the water cycle have consequential impacts on oceanic, groundwater, and land processes, influencing phenomena such as droughts and floods.

Precipitation deficits can also cause drought, which is a prolonged period of little to no rainfall. Droughts have impacts on the environment and humans, at times causing a chain reaction. For example, California had a drought from 2012 to 2014. While it isn't uncommon for California to have periods of low precipitation, that with a combination of sustained record high temperatures created severe water shortages. The drought subsequently dried out rivers which hosted populations of the Chinook salmon, affecting the population. That in turn affected Native American tribes food supply.

In this vignette, you will be combining the basic plot() function with sf functions to plot singular attributes, and using more advanced plotting functions such as ggplot() for plotting histograms, multi-panel plots, and a time series. 


## Load Data

We'll start again with the WSIM-GLDAS 12 month integration anomaly file from SEDAC and quickly subset it to the continental United States.

```{r warning=FALSE}
#| code-fold: true
# generate a vector of dates for subsetting
wsim_gldas <- stars::read_stars("composite_12mo.nc", proxy = FALSE)
keeps<-seq(lubridate::ymd("2000-12-01"),
           lubridate::ymd("2014-12-01"), 
           by = "year")

# filter using that vector
wsim_gldas <- dplyr::filter(wsim_gldas, time %in% keeps)

# you may want to clear your memory if your system is limited
gc()

# subset for deficit
wsim_deficit <- wsim_gldas['deficit']

usa <- httr::GET("https://www.geoboundaries.org/api/current/gbOpen/USA/ADM1/")
usa <- httr::content(usa)
usa <- sf::st_read(usa$gjDownloadURL)
drops<-
  c("Alaska", "Hawaii", 
    "American Samoa",
    "Puerto Rico",
    "Commonwealth of the Northern Mariana Islands", 
    "Guam", 
    "United States Virgin Islands")

usa<-usa[!(usa$shapeName %in% drops),]
wsim_deficit_usa<-wsim_deficit[usa]
```

Now we'll verify this with `print()` and `plot()`.

```{r}
print(wsim_deficit_usa)
```

The output shows that we've selected a single attribute ('deficit') and 15 time-steps in the 'time' dimension. Plot only the 14th time step and show the border color as purple, and a lineweight of 3. 

```{r warning=FALSE, message=FALSE}
wsim_deficit_usa |>
  dplyr::slice(index = 15, along = "time") |>
  plot(reset = FALSE, breaks = c(0,-5,-10,-20,-40,-50))

plot(sf::st_geometry(usa),
     add = TRUE,
     lwd = 3,
     fill = NA,
     border = 'purple')
```


## Exploratory Histogram

Create histogram of raster values for a single time step.

Get the values out of the raster and create a histogram.

```{r}
# filter for the first time-step in the file
usa1 <-
  wsim_deficit_usa |> dplyr::slice(time, 1)

# extract the values into a data.frame
usa1<-as.data.frame(as.numeric(wsim_deficit_usa$deficit))

# appropriately name the values (it was lost in the example)
names(usa1)<-"Deficit"
```


Check the values.

```{r}
ggplot2::ggplot(usa1, ggplot2::aes(Deficit))+
  ggplot2::geom_histogram(na.rm = TRUE)
```
There are some bad outliers, we can just zoom into the majority of values by setting x-axis limits.

```{r}
ggplot2::ggplot(usa1, ggplot2::aes(Deficit))+
  ggplot2::geom_histogram(na.rm = TRUE)+
  ggplot2::xlim(c(-60,50))
```

Extreme values or other items of note might require additional visualization or other data exploration.

## Multi-Panel Time Series

Create a multipanel time series of 12 month integration CONUSA; similar to what we used to identify our case studies. Each panel will represent 1 year.

Load in a CONUSA geojson from geoBoundaries. Copy methods from previous vignette.

```{r warning = TRUE}
# the histogram can be studied to properly choose breaks. Breaks chosen are [-60, -40, -20, 0, 20, 40, 50] from studying the histogram.

wsim_deficit_usa |>
  dplyr::slice(index = 1:14, along = "time") |>
  plot(reset = FALSE,
       col = leg_colors<-c(
    '#9B0039',
    # -50 to -40
    '#D44135',
    # -40 to -20
    '#FF8D43',
    # -20 to -10
    '#FFC754',
    # -10 to -5
    '#FFEDA3',
    # -5 to -3
    '#FFFFFF'), breaks = c(-60, -40, -20, 0, 20, 40, 60))
```

Plotting without breaks produces a similar but different map.

```{r}
wsim_deficit_usa |>
  dplyr::slice(index = 1:14, along = "time") |>
  plot(reset = FALSE,
       col = leg_colors<-c(
    '#9B0039',
    # -50 to -40
    '#D44135',
    # -40 to -20
    '#FF8D43',
    # -20 to -10
    '#FFC754',
    # -10 to -5
    '#FFEDA3',
    # -5 to -3
    '#FFFFFF'))
```

Once hot spots are easily identified pick a region of interest to zoom in on using the 1 month integration dataset.

Load in the 1 month integration dataset and subset/index the dataset to the region of interest (copy code from previous vignette). Use `dplyr::slice` or other method to pull out just the 12 months from the year of interest. Code demonstrating these techniques in previous vignette.

```{r}
gc()
wsim_gldas_1mo <- stars::read_stars("composite_1mo.nc", proxy = FALSE)

print(wsim_gldas_1mo)

# subset for USA
wsim_gldas_1mo<-wsim_gldas_1mo[usa]

# subset for deficit
wsim_gldas_1mo <- wsim_gldas_1mo['deficit']

# create a new sequence of dates, this time monthly, for the chosen year of 2011
keeps3<-seq(lubridate::ymd("2012-01-01"),
           lubridate::ymd("2012-12-01"), 
           by = "month")

wsim_gldas_1mo <- dplyr::filter(wsim_gldas_1mo, time %in% keeps3)
```


Create a multi-panel figure with each panel representing 1 month to identify the most intense months of drought or flooding. Starting with this one maybe use `ggplot` and a nice palette, legend, and panel headings. Will probably have to use some sort of faceting to make individual panels.

```{r}
# turn into dataframe
df_wsim_gldas_1mo <-as.data.frame(wsim_gldas_1mo, xy = TRUE) 

ggplot2::ggplot(data = df_wsim_gldas_1mo) +
  ggplot2::geom_raster(ggplot2::aes(x = x, y = y, fill = deficit)) +
  ggplot2::scale_fill_viridis_c(limits = c(-60, 60), option = "magma") +
  ggplot2::theme_void() +
  ggplot2::theme(legend.position = "bottom") +
  ggplot2::geom_sf(data = usa, fill = NA, color = "black") +
  ggplot2::facet_wrap(time ~ .)

```

Visualize an individual cell with particular extreme or maybe volatile values. Use Google Maps to identify the latitude/longitude of a place of interest. Maybe an urban center or other important location in the region that suffered from the extreme event.

Create a vector with the point location.

```{r}
# vector of coordinates for Austin, Texas
coords <- c(-97.7431, 30.2672)

# create a stars object with point coordinates where 'sf::st_sfc()' is used to create a geometry list column and add a coordinate reference system. 'sf::st_points()' is within that function to turn the coords vector into a point. 'sf:: st_crs()' and 'stars::st_dimensions()'  respectively retrieves the coordinate reference system and the dimensions of wsim_gldas_1mo to ensure that the final values in point_stars is the same reference system and shape. 

point_stars <- sf::st_sfc(sf::st_point(coords), crs = sf::st_crs(wsim_gldas_1mo),
                      dim = names(stars::st_dimensions(wsim_gldas_1mo)))
```


Use `stars::extract` to extract raster values in the stack at the point location. 

```{r}
extracted_vals <- stars::st_extract(wsim_gldas_1mo, point_stars)
```

The resulting data frame of time series values should be inspected. It may also need to be converted from wide format to long format so it may be plotted in `ggplot`. Use either pivot wider/longer from `dplyr` or cast/melt from `data.table`.

```{r}
# convert to dataframe
extracted_df <- as.data.frame(extracted_vals)

print(extracted_df)
```

Once in the proper format, plot using `ggplot`.

```{r}
ggplot2::ggplot(extracted_df, ggplot2::aes(x = time, y = deficit)) + ggplot2::geom_line()
```

Texas, in 2011, experienced a severe drought which caused rivers to dry up and lakes to reach historic low levels. The drought cost farmers and ranchers an estimated $8 billion in losses. Furthermore, the dry conditions fueled a series of wildfires across the state in early September. Which months experienced the most severe drought?

## Population Exposure Plot

Use Gridded Population of the World and `exactextractr` to determine the number of people exposed to a given anomaly for each month of the year. 

Gridded Population of the World is a dataset group in SEDAC which models the distribution of the global human population through counts and densities on a raster. These estimates are available for the years of 2000, 2005, 2015 and 2020. The Population Count dataset will be used for this example, and can be found here. [https://sedac.ciesin.columbia.edu/data/collection/gpw-v4]

'exactextractr' is an R package that summarizes raster values over groupings, or zones, also known as zonal statistics. Zonal statistics help in assessing the statistical characteristics of a certain region. The package can be used to sum all the cells in a polygon, i.e. a region; other functions include computing the mean, median, and other types of statistics. More information on the package can be found here [https://github.com/isciences/exactextractr]. 

Load in GPW data and the `exactextractr` package

```{r}
#gpw <- stars::read_stars("gpw_v4_population_count_rev11_2020_15_min.tif", proxy = FALSE)
#print(gpw)

#gpw_sf <- sf::st_as_sf(gpw)
#print(gpw_sf)

#gpw_df <-as.data.frame(gpw)
#print(gpw_df)

```
```{r}
gpw1 <- terra::rast("gpw_v4_population_count_rev11_2020_15_min.tif")
print(gpw1)

library(exactextractr)
#gpw_df1 <-as.data.frame(gpw1)
#plot(gpw1)
#print(gpw_df1)
```

Perform the time series zonal summary.

This might be a bit tricky; been a while for me. Have to look up the proper code. Dan has good examples on the exactextractr package website.

Resulting data.frame will probably need to be transformed to long (just like before), so it can be plotted.

```{r}

extracted_mean <- exact_extract(gpw1, usa, 'mean')
print(extracted_mean)

extracted_mean_df <- as.data.frame(extracted_mean)
print(extracted_mean_df)

#extracted_mean_df <- dplyr::mutate(extracted_mean_df, ID = dplyr::row_number())
#print(extracted_mean_df)
```
```{r}
joined_var = cbind(usa, extracted_mean)
print(joined_var)

joined_var_df = as.data.frame(joined_var)
print(joined_var_df)

```


Now plot the data in ggplot. I have some existing code I can pull to help with the plotting--or at least make it fancy.

```{r}

ggplot2::ggplot(data = joined_var_df, ggplot2::aes(x = shapeName, y = extracted_mean, group = 1)) + ggplot2::geom_line() +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 90, hjust = 1)) +
  ggplot2::theme(plot.title =  ggplot2::element_text(hjust = 0.5)) +
  ggplot2::ggtitle("Population Mean per State") + 
  ggplot2::xlab("State") + 
  ggplot2::ylab("Mean")
  


#plot(joined_var_df)

```
```{r}
#tp <- terra::extract(gpw1, usa, mean, na.rm = FALSE)
#print(tp)
#ggplot2::ggplot(extracted_df, ggplot2::aes(x = time, y = deficit)) + ggplot2::geom_line()
#ggplot2::ggplot(data = tp)
#ggplot2::geom_line(data = tp)
#ggplot2::ggplot(data = tp, ggplot2::aes(x = ID, y = gpw_v4_population_count_rev11_2020_15_min)) + ggplot2::geom_line()

```

## Conlusion

The insights drawn from various data sources, including remote sensing data, contribute to the understanding of water availability. Models like the Global Land Data Assimilation System (GLDAS) utilize satellite and ground-based observations to provide real-time, high-resolution data on land surface states and fluxes, aiding in a more comprehensive understanding of drought and flooding.
